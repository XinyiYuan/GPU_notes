# 分布式训练

[TOC]

## Prerequisites

存在的问题：Some models can be trained only with a very small batch size on a single GPU. Some models may not fit on single GPU.

=> This makes the training very inefficient and impossible in some cases.

方法：**Data parallelism** and **Model parallelism**

- Data parallelism

适用场景：The model fits completely in to the GPU memory。但是batch-size受限。

解决方法：在不同GPU上运行不同的实例（instance）。模型的每个实例都使用相同的参数进行初始化，但在前向传递过程中，每个模型会收到不同的数据，彼此之间可以独立计算。每隔一段时间（比如一个batch或者若干个batch）后，需要彼此之间同步模型权重。

> 所有模型的参数是相同的

- Model parallelism

适用场景：The model doesn’t fit on a single GPU

解决方法：Split the model on multiple GPUs（具体怎么操作？）

分为三种：Inter-layer Partition (Pipeline Parallelism); Intra-layer Partition (Tensor Parallelism); Sparse Activation (Sequence Parallelism).

存在问题：利用率低。任意时刻只有一个GPU在工作，其他都闲置。

<img src=".\pic\wechat20230918175114.png" alt="wechat20230918175114" style="zoom:30%;" />

<img src=".\pic\wechat_20230919154722.png" alt="wechat_20230919154722" style="zoom:40%;" />

补充一个概念解释：orthogonal正交 和 complimentary互补

其实就是上面的两个图，竖向切了一刀vs.横向切了一刀，那么这两个切法，就是“正交的”（夹角90度）。而且这两个方法可以同时在一个代码中实现，具有“互补”的关系，即：

## State-of-arts

### Data Parallelism

#### Zero Redundancy Optimizer (ZeRO) (SC'20)

> ZeRO: memory optimizations toward training trillion parameter models, https://dl.acm.org/doi/10.5555/3433701.3433727

- 在模型训练期间，大部分内存被以下三种情况之一消耗：

  - 激活

    业界已有方案（如论文Training deep nets with sublinear memory cost），可以以33%的重新计算开销为代价，几乎可以消除激活所需的所有内存。

    因此主要考虑以下两点。

  - OGP状态

    O：优化器状态（例如Aadam优化器中的的momemtum、variance）

    P：参数

    G：梯度

    > 以ADAM为例：使用ADAM对具有Ψ个参数的模型进行混合精度训练需要足够的内存来保存参数和梯度的fp16副本，内存要求分别为2Ψ和2Ψ字节。此外，它还需要保存优化器状态：参数、Momentum和Variance的fp32副本，内存需求分别为4Ψ、4Ψ和4Ψ字节。让我们用K来表示优化器状态的内存乘数，也就是说，存储它们所需的额外内存是KΨ字节，混合精度ADAM的K=12。总的来说，这将导致2Ψ+2Ψ+KΨ=16Ψ字节的内存需求。**对于一个拥有15亿（1.5G）个参数的GPT-2这样的模型，这导致了至少24GB的内存需求，远远高于单独保存fp16参数（2Ψ）所需的3GB内存。**

  - 临时缓冲区。

- 基于以下三个关键的观察：

  - 数据并行比模型并行具有更好的伸缩效率，因为模型并行减少了计算的粒度，同时也增加了通信开销。

  - 数据并行缺乏内存效率。

  - 数据和模型并行都保持了整个训练过程中所需的所有模型状态，但并不是所有的时间都是必需的。例如，仅在某个层的正向传播和反向传播期间才需要与每个层对应的参数。

- 内存优化方案和通信开销：

  Ψ表示模型大小（参数个数），K表示优化器状态的内存乘数（参考上面例子），Nd表示数据并行度（GPU个数）。此处假设Ψ=75亿，Nd=64，K=12。

  - ZeRO-1：Pos（优化器状态）

    在每个gpu中保存全部的参数和梯度，但是只保存1/Nd的优化器变量，这将导致总的内存消耗变为2Ψ+2Ψ+12Ψ/Nd

    非常适合使用类似Adam进行优化的模型训练，因为Adam拥有额外的参数m（momentum）与v（variance），特别是FP16混合精度训练。但不适合使用SGD类似的优化器进行模型训练，因为SGD只有较少的参数内存，并且由于需要更新模型参数，导致额外的通讯成本。

  - ZeRO-2：Pos+g（优化器状态+梯度）

    每个gpu中只保存1/Nd的梯度，这将导致总的内存消耗变为2Ψ+（2Ψ+12Ψ）/Nd

  - ZeRO-3：Pos+g+p（优化器状态+梯度+参数）

    每个gpu中只保存1/Nd的参数，这将导致总的内存消耗变为（2Ψ+2Ψ+12Ψ）/Nd

  对于正常的all-reduce更新的数据并行的模型训练，一次完整的迭代包含reduce-scatter和all-gather 两次操作，分别需要Ψ的通信量，每gpu共计消耗2Ψ通信量。对于Pos和Pos+g，同样需要对梯度进行reduce-scatter和all-gather，所以通信量同样是每gpu共计消耗2Ψ。

  对于Pos+g+p，需要对梯度进行一次reduce-scatter操作（因为每个gpu各自负责部分参数的更新，因此不需要对梯度进行all-gather操作），对参数需要进行正向和反向两次传递，所以需要消耗2Ψ通信量，共计每gpu消耗3Ψ通信量。所以Pos+g+p的通信开销是正常的all-reduce或者Pos+g的1.5倍。

- 优化后的训练步骤（以Pos+g+p为例）：

  - 初始状态：每个gpu保存总参数的1/Nd，并且保存这些参数对应的梯度、优化器状态(P_n,G_n,O_n)。参数可以按层划分。每个gpu同时还负责分配到自己身上的数据data（数据并行）。
  - 正向计算第一层时，gpu_n将自己负责的参数(P_n)广播给其它所有的gpu；后面的模型层以此类推；最后每个gpu_n获得自己对应数据data_n的loss_n。
  - 进行反向计算，此时需要gpu_n将自己负责的参数(P_n)广播给其它所有的gpu，最后计算得到对应于数据data_n的梯度。
  - 将第二步的梯度聚合到对应的gpu_n上，每个gpu负责更新自己的P_n,G_n,O_n。
  - 进行下一次迭代

  从这个训练流程，我们可以看出，传递参数过程中它使用了广播的方式，而梯度聚合过程则使用了类似Allreduce中reduce-scatter的模式，但不再需要进行all-gather，因为每个gpu只需要更新自己负责的部分参数。

#### Fully Sharded Data Parallel (FSDP)

> Fully Sharded Data Parallel (FSDP): https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html

FSDP是pytorch1.11的新特性。其新特性目的主要是训练大模型。我们都知道pytorch DDP用起来简单方便，但是要求整个模型能加载一个GPU上，这使得大模型的训练需要使用额外复杂的设置进行模型拆分。pytorch的FSDP从DeepSpeed ZeRO以及FairScale的FSDP中获取灵感，打破模型分片的障碍（包括模型参数，梯度，优化器状态），同时仍然保持了数据并行的简单性。

DP v.s. DDP:

- DP (Data Parellelism)：目前由于性能问题基本不用了
  - 单进程多线程模式，由于锁的机制导致线程间同步存在瓶颈。
  - 使用普通的All-Reduce机制，所有的卡需要将梯度同步给0号节点，并由0号节点平均梯度后反向传播，再分发给所有其他节点，意味着0号节点负载很重。
  - 由于2的原因，导致0号gpu通讯成本是随着gpu数量的上升而线性上升的。
  - 不支持多机多卡。
- DDP (Distributed Data Parallel)：基于Ring-All-Reduce通讯算法的数据并行策略
  - 负载分散在每个gpu节点上，所以每个节点的通讯时间基本是一致的。并且不需要通过0号gpu分发全模型的参数到每个gpu上。
  - 使用ring-all-reduce的方式进行通讯，随着 GPU 数量 N 增加，总传输量恒定。也就是理论上，随着gpu数量的增加，ring all-reduce有线性加速能力。
  - 多进程的方式，突破锁的束缚，相比多线程在容灾以及拓展上更有优势。

Pytorch的FSDP是一种数据并行的训练方法（实际上就是ZeRO-3）。传统的数据并行会在每个GPU上维护一份模型参数，梯度，优化器状态的副本，但是FSDP将这些状态分片到所有的数据并行worker中，并且可以选择将分片的模型参数卸载到CPU上。

通常，模型层以嵌套方式用 FSDP 包装，因此在前向或后向计算期间，只有单个 FSDP 实例中的层需要将完整参数收集到单个设备。聚合到的完整参数会在计算后立即释放，释放的内存可以用于下一层的计算。通过这种方式，可以节省峰值 GPU 内存，因此可以扩展训练以使用更大的模型大小或更大的批量大小。为了进一步最大化内存效率，当实例在计算中不活动时，FSDP 可以将参数、梯度和优化器状态卸载到 CPU。

<img src=".\pic\wechat_20230919151837.png" alt="wechat_20230919151837" style="zoom:30%;" />

### Model Parallelism

#### Pipeline Parallelism

> https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html

pipeline parallelism是比较常见的模型并行算法，它是模型做层间划分，即inter-layer parallelism。以下图为例，如果模型原本有6层，你想在2个GPU之间运行pipeline，那么每个GPU只要按照先后顺序存3层模型即可。它们的主要目的都是降低bubble time。

<img src=".\pic\wechat20230919103153.png" alt="wechat20230919103153" style="zoom:30%;" />

#### Tensor Parallelism

前面介绍的Pipeline Parallelism是对模型层间做划分，叫inter-layer parallelism。那么另一种方式则是对模型层内做划分，即intra-layer Parallelism，也叫Tensor Parallelism。

**1D Tensor Parallelism**

> Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (2019), https://arxiv.org/abs/1909.08053v4
>
> code: https://github.com/NVIDIA/Megatron-LM

由Nvidia发表的文章。提出一种针对transformer的模型并行方式，可用来训练参数规模达到十亿级别的语言模型。

**2D Tensor Parallelism**

> An Efficient 2D Method for Training Super-Large Deep Learning Models (IPDPS'23), https://ieeexplore.ieee.org/abstract/document/10177476)

基于SUMMA和Cannon矩阵相乘算法沿着两个不同的维度对 输入数据、模型权重、每层的输出 进行划分。

每个GPU都只会保存部分的输入输出以及部分的权重。虽然相比于1D Tensor并行，2D额外增加了模型权重的通信，但是需要注意的是当GPU数量很多的时候，每个GPU上分配的模型权重就会小很多，而且因为使用的All-reduce通信方式，所以2D也还是要比1D更高效的。

**2.5D Tensor Parallelism**

> 2.5-dimensional distributed model training (2021), https://arxiv.org/abs/2105.14500v1
>
> Tesseract: Parallelize the Tensor Parallelism Efficiently (ICPP '22), https://dl.acm.org/doi/abs/10.1145/3545008.3545087
>
> 以上两个是同一篇

受[2.5D矩阵乘法算法](E. Solomonik and J. Demmel, “Communication-optimal parallel 2.5d matrix multiplication and lu factorization algorithms,” in Euro-Par, 2011)启发进一步对2D Tensor并行的优化。具体来说2.5D增加了depth维度。当depth=1时等价于2D。一般来说至少需要8个GPU才能运行2.5D算法。

**3D Tensor Parallelism**

> Maximizing Parallelism in Distributed Training for Huge Neural Networks (2021), https://arxiv.org/abs/2105.14450

基于[3D矩阵乘法算法](R. C. Agarwal, S. M. Balle, F. G. Gustavson, M. Joshi, and P. Palkar, “A three-dimensional approach to parallel matrix multiplication,” IBM Journal of Research and Development, vol. 39, no. 5, pp. 575–582, 1995.)实现的。3D Tensor并行的通信开销复杂度是$O(N^\frac{1}{3})$，计算和内存开销都均摊在所有GPU上。

#### Sequence Parallelism

> Sequence Parallelism: Long Sequence Training from System Perspective (ACL'23), https://aclanthology.org/2023.acl-long.134/

Tensor parallelism主要是为了解决由 model data（模型权重，梯度和优化器状态）导致的内存瓶颈，但是non-model data也可能成为性能瓶颈。比如像AlphaFold和NAS任务中会存在很多中间特征值（也叫activations）。Sequential Parallelism就为了解决non-model data导致的性能瓶颈而提出的。

> 以DARTS算法为例，它的模型参数量其实并不多，但是它有很多分支，所以activations会消耗大量GPU内存，这也是为什么很多NAS算法只能在CIFAR-10上搜索到合适的模型结构后，再做人工扩展，最后应用到ImageNet上做性能验证。同样地，在使用Transformer训练语言模型时，由于Transformer层中的Self-attention机制的复杂度是$O(n^2)$，其中n是序列长度。换言之，长序列数据将增加中间activation内存使用量，从而限制设备的训练能力。

本文主要聚焦于transformer结构的self-attention和mlp两个结构，通过将序列切分到多个计算设备，并行计算，解决长序列训练的问题。在 self-attention 部分，由于Q、K、V三者是互相交互的关系，所以在实现上借鉴了Ring-Allreduce算法实现，实现了RingQK和RingAV两种计算方式。具体参考原文。

## Frameworks

### Megatron-LM

> Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (2019), https://arxiv.org/abs/1909.08053v4
>
> code: https://github.com/NVIDIA/Megatron-LM

NVIDIA

支持transformer模型的模型并行（张量、序列和管道）和多节点预训练，同时还支持 BERT、GPT 和 T5 等模型

### DeepSpeed

> code: https://github.com/microsoft/DeepSpeed
>
> tutorials: https://www.zhihu.com/people/deepspeed/posts

Windows

使用的并行算法：ZeRO

> 节省内存来自两方面（细节可以看论文）：1，对optimizer 阶段的重构，从原来上来就allreduce，各个节点都保持一份weight, gradient以及adam 需要mean, var等，变成只在一个节点保存，通信变成了reduce和broadcast；2，各个层次的参数生命周期不重叠，每个节点仅保存计算时需要的参数。
>
> DeepSpeed实质上仍是数据并行， 和 Nvidia Megatron的模型并行相比有优势，原因是Megatron在不应该用模型并行的地方使用了模型并行。举个例子，如果某一个layer的参数量巨大，大到一块GPU装不下，或者即使装的下，使用DeepSpeed 通信量也比模型并行高的话，模型并行仍是最优选择。

### FairScale

> code: https://github.com/facebookresearch/fairscale

Facebook

使用的并行算法：ZeRO

为了减少模型状态的冗余，提出了三种不同的算法，均已在 FairScale 中实现：Optimizer State Sharding (OSS)，Sharded Data Parallel (SDP)，Fully Sharded Data Parallel (FSDP)。

FairScale支持的FSDP是扩展大型神经网络训练的推荐方式。

### ParallelFormers

> code: https://github.com/tunib-ai/parallelformers

一个基于Megatron-LM的库。

它与Huggingface库很好地集成在一起。Huggingface库中的模型可以用一行代码并行化。

目前它只支持推理。

### Colossal-AI

> paper: https://arxiv.org/abs/2110.14883
>
> code: https://github.com/hpcaitech/ColossalAI

提供了一组并行组件，可以用来实现定制化的分布式/并行训练，包含以下并行化策略和增强功能：

- Data Parallelism
- Pipeline Parallelism
- 1D,2D,2.5D,3D Tensor Parallelism
- Sequence Parallelism
- Zero Redundancy Optimizer (ZeRO)
- Heterogeneous Memory Management (PatrickStar)
- For Inference Energon-AI

### Alpa

> code: https://github.com/alpa-projects/alpa

### Hivemind

> Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices (NeurIPS'21), https://papers.nips.cc/paper_files/paper/2021/hash/97275a23ca44226c9964043c8462be96-Abstract.html
>
> code: https://github.com/learning-at-home/hivemind

在互联网上使用Pytorch进行去中心化深度学习的库。它主要服务场景是在来自不同大学、公司和志愿者的数百台计算机上训练一个大型模型。

其主要特点是：

- 没有主节点的分布式训练：分布式哈希表允许连接分散网络中的计算机。
- 容错反向传播：即使某些节点没有响应或响应时间过长，前向和后向传递也会成功。
- 分散的参数平均：迭代地聚合来自多个工作人员的更新，而无需在整个网络中同步。
- 训练任意大小的神经网络：它们的部分层通过分散的专家混合分布在参与者之间。

### OneFlow

> code: https://github.com/Oneflow-Inc/oneflow

一个深度学习框架，旨在实现用户友好、可扩展和高效。 使用 OneFlow，很容易：

- 使用类似 PyTorch 的 API 编写模型
- 使用 Global View API 将模型缩放到 n 维并行/分布式执行
- 使用静态图编译器加速/部署模型。

### Mesh-Tensorflow

> code: https://github.com/tensorflow/mesh

是一种用于分布式深度学习的语言，能够指定广泛的分布式张量计算类别。

这里的“Mesh”是指处理器或计算设备的互连网络。

### EPL

> paper: Whale: Efficient Giant Model Training over Heterogeneous GPUs (ATC'22), https://www.usenix.org/conference/atc22/presentation/jia-xianyan
>
> slides: https://www.usenix.org/sites/default/files/conference/protected-files/atc22_slides_jia_xianyan.pdf
>
> code: https://github.com/alibaba/EasyParallelLibrary

Alibaba

Challenges: Inefficiency in utilizing heterogeneous GPUs (e.g., Computing Pool with GPUs: P100, V100, A100 and etc)

Gaps and Opportunities:

- Lack unified abstraction to support all of the parallel strategies and the hybrids
  - Unified abstraction for strategy expression
- Fully automatic parallel strategy has high cost for giant models
  - Incorporate user hints
- Inefficiency in utilizing heterogeneous GPUs
  - Parallel strategies should be used adaptively and dynamically
- Require significant model code refactoring
  - Minimize code change, switch among strategies easily

支持Data Parallelism，Pipeline Parallelism，和Tensor Model Parallelism。需要对源代码进行一些修改。

EPL框架：

- 接口层：用户的模型编程接口基于TensorFlow，同时EPL提供了易用的并行化策略表达接口，让用户可以组合使用各种混合并行策略；

- 中间表达层：将用户模型和并行策略转化成内部表达，通过TaskGraph、VirtualDevices和策略抽象来表达各种并行策略；

- 并行化引擎层：基于中间表达，EPL会对计算图做策略探索，进行显存/计算/通信优化，并自动生成分布式计算图；

- Runtime执行引擎：将分布式执行图转成TFGraph，再调用TF 的Runtime来执行。

<img src=".\pic\wechat20230920114532.png" alt="wechat20230920114532" style="zoom:50%;" />

### Bagua

> paper: Bagua: Scaling up Distributed Learning with System Relaxations (PVLDB'22), https://dl.acm.org/doi/10.14778/3503585.3503590
>
> code: https://github.com/BaguaSys/bagua

从单机单卡的训练到多机多卡训练的核心，是每个卡把自己的计算结果进行累加和传播。

Bagua 将通信过程抽象成了如下的算法选项：

- 中心化或是去中心化（Centralized or Decentralized）：在中心化的通讯模式中，梯度或模型的同步过程需要所有的工作节点进行参与，因此，较高的网络延时往往会导致训练效率的降低。去中心化的通信模式往往可以有效解决这一问题：在该模式下，工作节点可以被连接成特定的拓扑结构（例如环），在通信过程中，每一个工作节点只与和它相邻的节点进行通信。
- 同步或是异步（Synchronous or Asynchronous）：同步模式中，在每一次迭代过程中，所有工作节点都需要进行通信，并且下一步迭代必须等待当前迭代的通信完成才能开始。反之，异步式分布算法则不需要等待时间：当某个节点完成计算后就可直接传递本地梯度，进行模型更新。
- 完整精度模式或信息压缩模式（Full-Precision or Low-Precision）：完整精度模式，会使用与本地模型相同的 32 位浮点数（float32）进行传输。另一方面，在通讯存在瓶颈的情况下，基于大量已有研究通过量化 (quantization) 或稀疏化 (sparsification) 等方法压缩梯度，再用压缩后的梯度更新参数。在很多场景下，可以达到和完整精度相同的精度，同时提升通讯效率。

虽然为了提升通讯效率，Bagua 没有依照传统的方式同步所有计算节点的结果，甚至每次同步的信息还有偏差，但是得益于最新理论上的进展，这几种通讯策略以及他们的组合最终收敛解的正确性和效率仍然能得到充分保证，而且计算复杂度跟同步中心化和信息无损的方法相当，但是通讯效率更高。