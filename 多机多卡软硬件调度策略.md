# 多机多卡软硬件调度策略

## 硬件层面

这里列举的方法比较底层，从pcie层面或驱动程序层面进行干涉

### SR-IOV

Single-root input/output virtualization，支持单个物理PCIe设备虚拟出多个虚拟PCIe设备，然后将虚拟PCIe设备直通到各虚拟机，以实现单个物理PCIe设备支撑多虚拟机的应用场景。

SR-IOV协议引入了两种类型功能的概念：物理功能（Physical Function, PF）和虚拟功能（Virtual Function, VF）。每个PF有标准的PCIe功能，能关联到多个VF。而每个VF都有与性能相关的资源，共享一个物理设备。简单来说，PF具有完整的PCIe功能，VF能独立使用关键功能。

> 详细介绍：https://zhuanlan.zhihu.com/p/465123778
>
> 性能实验（网卡）：https://zhuanlan.zhihu.com/p/630066202

什么设备适合实现SR-IOV？需要满足：

- 硬件资源容易partition
- 无状态（或接近无状态）

因此，常见PCIe设备中，最适合SR-IOV的就是网卡。

应用到GPU时存在的困难：虽然基本是无状态的，但硬件复杂度极高，partition很难实现。

### MPT (Mediated Pass-Through) 受控直通

> nVidia GRID vGPU、Intel GVT-g（KVMGT、XenGT）的实现思路

基本思路：

- 敏感资源，如配置空间，是虚拟的
- 关键资源，如MMIO中CSR部分，是虚拟的
- 性能关键资源，如MMIO中其他部分，硬件partition后直接分给VM
- Host上必须存在一个virtualization-aware的驱动程序，负责模拟和调度，实际上是vGPU的device-model

优点：1:N灵活性，高性能，渲染计算媒体的功能完整性

缺点：调试困难，必须有一个pGPU驱动，负责vGPU的模拟和调度工作

## 软件层面

### API转发

通过拦截程序发出的GPU申请，在软件层面实现“GPU虚拟化”。以AWS Elastic GPU为例：

- VM中看不到真的或假的GPU，但可以调用OpenGL API进行渲染
- 在OpenGL API层，软件捕捉到该调用，转发给Host
- Host请求GPU进行渲染
- Host把渲染结果转发给VM

优点：灵活，不依赖于GPU厂商，不限于系统虚拟化环境

缺点：复杂度高，功能不完整，性能无保证

采用此方法的框架：

- **GaiaGPU（腾讯tke，完全开源）**

  > GaiaGPU: Sharing GPUs in Container Clouds (ISPA'18) 
  >
  > code: https://github.com/tkestack/gpu-manager

- KubeShare（开源）

  > KubeShare: A Framework to Manage GPUs as First-Class and Shared Resources in Container Cloud (HPDC'20)
  >
  > code: https://github.com/NTHU-LSALAB/KubeShare

- rCUDA（未开源）

  > rCUDA: Reducing the Number of GPU-Based Accelerators in High Performance Clusters (HPCS'10)
  >
  > http://www.rcuda.net

### 其他算法

这里列举的算法聚焦于GPU效率/利用率的提升（偏研究，大部分没开源）

- SIREN

  > Distributed Machine Learning with a Serverless Architecture (INFOCOM'19) 

  一个基于无服务器架构设计的分布式机器学习框架。由两部分组成：本地客户端和无服务器云平台（AWS Lambda）。本地客户端使用深度强化学习agent进行资源调度决策，云平台根据决策结果为ML训练任务加载无状态函数。

  工作流程：

  本地上传一个代码包（code package，包含用户定义的ML模型和依赖的库函数）到云平台

  step1.本地scheduler给出初始化资源方案（resource scheme），云平台根据方案加载无状态函数集群

  step2.云端函数集群进行基于SGD的第一个epoch的训练

  step3.云平台收集作业的函数状态数据，并返回给本地Function Manager

  step4.Function Manager将函数状态发送给DRL Agent（已经过训练）

  step5.DRL agent为下一个epoch做出资源调度方案（resource scheme），并发送给scheduler

  <img src="./GPU+container/pic/WechatIMG480.jpg" alt="WechatIMG480" style="zoom:30%;" />

  特点：每个epoch都会用agent进行一次决策。所以在不同epoch中可以启动不同数量、不同内存配置的函数。解决了ML工作流中不同任务异构性导致的资源不平衡问题。ML用户往往会根据整个工作流中需要最大算力的时刻进行资源分配，导致GPU利用率降低。

- Gandiva, AntMan

  > 同一作者「肖文聪」（Wencong Xiao，[https://wencongxiao.github.io/](https://link.zhihu.com/?target=https%3A//wencongxiao.github.io/)）的两篇文章，目的类似，解决深度学习的 GPU 集群资源使用率低的问题

  > Gandiva: Introspective Cluster Scheduling for Deep Learning (OSDI ’18)
  >
  > slide: https://www.usenix.org/sites/default/files/conference/protected-files/osdi18_slides_sivathanu.pdf

  > AntMan: Dynamic Scaling on GPU Clusters for Deep Learning(OSDI'20) https://www.usenix.org/system/files/osdi20-xiao.pdf
  >
  > slide: https://www.usenix.org/sites/default/files/conference/protected-files/osdi20_slides_xiao.pdf

  利用领域特定知识domain-specific knowledge，从深度学习训练任务的特点入手：Feedback-driven exploration（反馈驱动的搜索），Heterogeneity（异构性），Intra-job predictability（作业内的可预测性）。Gandiva利用特征3来解决由特征1.2导致的GPU利用率低的问题。

  调度机制：Suspend-Resume（类似轮流运行），Packing（类似任务并行），Migration（任务迁移到另一个GPU上），Grow-Shrink（存在空闲时给任务分配空余GPU）。

  性能衡量：Profiling（比对每个迭代过程所花费的时间，时间缩短则说明策略有效，否则说明策略无效，取消该调度并进行下一次尝试。trial-and-error，而不是建模计算）。